import streamlit as st
from streamlit_ketcher import st_ketcher
import pandas as pd
import os
import re
import glob
import joblib
import random
import string
import io
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any
import numpy as np

# æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦åº“
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import xgboost as xgb
import lightgbm as lgb
from tensorflow import keras
from tensorflow.keras import layers

# å¯è§†åŒ–åº“
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff

# åŒ–å­¦ä¿¡æ¯å­¦åº“
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem, rdFingerprintGenerator, Descriptors, rdMolDescriptors
from rdkit.Chem.Scaffolds import MurckoScaffold

# SHAPè§£é‡Šæ€§AI
import shap

# ç”Ÿç‰©ä¿¡æ¯å­¦å’ŒAPI
from Bio import Entrez
from openai import OpenAI

# PDFç”Ÿæˆ
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter

# é…ç½®é¡µé¢
st.set_page_config(
    page_title="2025 CADDè¯¾ç¨‹å®è·µå¹³å°",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    /* ä¸»é¢˜é…è‰² */
    :root {
        --primary-color: #1f4e79;
        --secondary-color: #2e8b57;
        --accent-color: #4CAF50;
        --background-color: #f8fafc;
        --text-color: #2c3e50;
    }
    
    /* éšè—é»˜è®¤å…ƒç´  */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    
    /* ä¸»å®¹å™¨æ ·å¼ */
    .main-container {
        padding: 2rem;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 15px;
        margin: 1rem 0;
        color: white;
        text-align: center;
    }
    
    /* å¡ç‰‡æ ·å¼ */
    .info-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        margin: 1rem 0;
        border-left: 4px solid var(--primary-color);
    }
    
    /* æŒ‡æ ‡å¡ç‰‡ */
    .metric-card {
        background: linear-gradient(45deg, var(--primary-color), var(--secondary-color));
        color: white;
        padding: 1rem;
        border-radius: 8px;
        text-align: center;
        margin: 0.5rem;
    }
    
    /* æŒ‰é’®æ ·å¼ */
    .stButton > button {
        background: linear-gradient(45deg, var(--primary-color), var(--secondary-color));
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.5rem 1rem;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    }
    
    /* ä¾§è¾¹æ æ ·å¼ */
    .css-1d391kg {
        background: linear-gradient(180deg, var(--primary-color), var(--secondary-color));
    }
    
    /* è¿›åº¦æ¡æ ·å¼ */
    .stProgress > div > div > div > div {
        background: linear-gradient(45deg, var(--accent-color), var(--secondary-color));
    }
    
    /* è¡¨æ ¼æ ·å¼ */
    .dataframe {
        border-radius: 8px;
        overflow: hidden;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    }
</style>
""", unsafe_allow_html=True)

# === å·¥å…·å‡½æ•°å’Œé…ç½® ===

class CADDConfig:
    """CADDå¹³å°é…ç½®ç±»"""
    def __init__(self):
        self.models = {
            "éšæœºæ£®æ—": RandomForestClassifier,
            "æ”¯æŒå‘é‡æœº": SVC,
            "XGBoost": xgb.XGBClassifier,
            "LightGBM": lgb.LGBMClassifier,
            "ç¥ç»ç½‘ç»œ": "neural_network"
        }
        
        self.model_params = {
            "éšæœºæ£®æ—": {
                "n_estimators": [50, 100, 200],
                "max_depth": [3, 5, 10, None],
                "max_features": ["sqrt", "log2"]
            },
            "æ”¯æŒå‘é‡æœº": {
                "C": [0.1, 1, 10],
                "kernel": ["rbf", "linear"],
                "gamma": ["scale", "auto"]
            },
            "XGBoost": {
                "n_estimators": [50, 100, 200],
                "max_depth": [3, 5, 7],
                "learning_rate": [0.01, 0.1, 0.2]
            },
            "LightGBM": {
                "n_estimators": [50, 100, 200],
                "max_depth": [3, 5, 7],
                "learning_rate": [0.01, 0.1, 0.2]
            }
        }
        
        # åŠ¨æ€æ£€æµ‹å¯ç”¨çš„æè¿°ç¬¦
        self.descriptors = self._get_available_descriptors()
    
    def _get_available_descriptors(self):
        """åŠ¨æ€æ£€æµ‹å¯ç”¨çš„åˆ†å­æè¿°ç¬¦"""
        base_descriptors = [
            "MolWt", "LogP", "NumHDonors", "NumHAcceptors", 
            "TPSA", "NumRotatableBonds", "NumAromaticRings",
            "HeavyAtomCount", "RingCount"
        ]
        
        # æ£€æŸ¥ FractionCsp3 æ˜¯å¦å¯ç”¨
        available_descriptors = base_descriptors.copy()
        
        # æµ‹è¯•åˆ†å­ç”¨äºæ£€æŸ¥æè¿°ç¬¦å¯ç”¨æ€§
        test_mol = Chem.MolFromSmiles("CCO")  # ç®€å•çš„ä¹™é†‡åˆ†å­
        
        if test_mol:
            # æ£€æŸ¥ FractionCsp3 æ˜¯å¦å¯ç”¨
            try:
                if hasattr(Descriptors, 'FractionCsp3'):
                    Descriptors.FractionCsp3(test_mol)
                    available_descriptors.append("FractionCsp3")
                elif hasattr(rdMolDescriptors, 'CalcFractionCsp3'):
                    rdMolDescriptors.CalcFractionCsp3(test_mol)
                    available_descriptors.append("FractionCsp3")
                else:
                    # ä½¿ç”¨æ‰‹åŠ¨è®¡ç®—
                    available_descriptors.append("FractionCsp3")
            except:
                # FractionCsp3 ä¸å¯ç”¨ï¼Œè·³è¿‡
                pass
        
        return available_descriptors

@st.cache_data
def load_config():
    """åŠ è½½é…ç½®"""
    return CADDConfig()

# === åˆ†å­å¤„ç†å‡½æ•° ===

@st.cache_data
def calculate_molecular_descriptors(smiles: str) -> Dict[str, float]:
    """è®¡ç®—åˆ†å­æè¿°ç¬¦"""
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return {}
    
    descriptors = {}
    
    # å®šä¹‰æè¿°ç¬¦è®¡ç®—å‡½æ•°åˆ—è¡¨ï¼ŒåŒ…å«åç§°å’Œè®¡ç®—å‡½æ•°
    descriptor_functions = [
        ('MolWt', lambda m: Descriptors.MolWt(m)),
        ('LogP', lambda m: Descriptors.MolLogP(m)),
        ('NumHDonors', lambda m: Descriptors.NumHDonors(m)),
        ('NumHAcceptors', lambda m: Descriptors.NumHAcceptors(m)),
        ('TPSA', lambda m: Descriptors.TPSA(m)),
        ('NumRotatableBonds', lambda m: Descriptors.NumRotatableBonds(m)),
        ('NumAromaticRings', lambda m: Descriptors.NumAromaticRings(m)),
        ('HeavyAtomCount', lambda m: Descriptors.HeavyAtomCount(m)),
        ('RingCount', lambda m: Descriptors.RingCount(m)),
    ]
    
    # FractionCsp3 éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆç‰ˆæœ¬å…¼å®¹æ€§ï¼‰
    def calculate_fraction_csp3(mol):
        """è®¡ç®—FractionCsp3ï¼Œå¤„ç†ç‰ˆæœ¬å…¼å®¹æ€§"""
        try:
            # é¦–å…ˆå°è¯•ä» Descriptors æ¨¡å—
            if hasattr(Descriptors, 'FractionCsp3'):
                return Descriptors.FractionCsp3(mol)
            # å°è¯•ä» rdMolDescriptors æ¨¡å—
            elif hasattr(rdMolDescriptors, 'CalcFractionCsp3'):
                return rdMolDescriptors.CalcFractionCsp3(mol)
            # æ‰‹åŠ¨è®¡ç®— FractionCsp3
            else:
                return calculate_csp3_fraction_manual(mol)
        except:
            return None
    
    # è®¡ç®—å¸¸è§„æè¿°ç¬¦
    for desc_name, desc_func in descriptor_functions:
        try:
            descriptors[desc_name] = desc_func(mol)
        except Exception as e:
            st.warning(f"è®¡ç®—æè¿°ç¬¦ {desc_name} æ—¶å‡ºé”™: {e}")
            descriptors[desc_name] = None
    
    # è®¡ç®— FractionCsp3
    try:
        frac_csp3 = calculate_fraction_csp3(mol)
        if frac_csp3 is not None:
            descriptors['FractionCsp3'] = frac_csp3
        else:
            st.info("FractionCsp3 æè¿°ç¬¦æš‚ä¸å¯ç”¨ï¼Œå·²è·³è¿‡")
    except Exception as e:
        st.warning(f"è®¡ç®— FractionCsp3 æ—¶å‡ºé”™: {e}")
    
    # ç§»é™¤ None å€¼
    descriptors = {k: v for k, v in descriptors.items() if v is not None}
    
    return descriptors

def calculate_csp3_fraction_manual(mol):
    """æ‰‹åŠ¨è®¡ç®— FractionCsp3"""
    try:
        sp3_carbons = 0
        total_carbons = 0
        
        for atom in mol.GetAtoms():
            if atom.GetAtomicNum() == 6:  # ç¢³åŸå­
                total_carbons += 1
                if atom.GetHybridization() == Chem.HybridizationType.SP3:
                    sp3_carbons += 1
        
        if total_carbons == 0:
            return 0.0
        return sp3_carbons / total_carbons
    except:
        return None

@st.cache_data
def mol_to_fingerprint(smiles: str, fp_type: str = "morgan") -> Optional[np.ndarray]:
    """ç”Ÿæˆåˆ†å­æŒ‡çº¹"""
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    
    try:
        if fp_type == "morgan":
            fpgen = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=2048)
            fp = fpgen.GetFingerprint(mol)
            arr = np.zeros((2048,))
            DataStructs.ConvertToNumpyArray(fp, arr)
            return arr
        elif fp_type == "maccs":
            fp = rdMolDescriptors.GetMACCSKeysFingerprint(mol)
            arr = np.zeros((167,))
            DataStructs.ConvertToNumpyArray(fp, arr)
            return arr
    except Exception as e:
        st.error(f"ç”ŸæˆæŒ‡çº¹æ—¶å‡ºé”™: {e}")
        return None

@st.cache_data
def get_scaffold(smiles: str) -> Optional[str]:
    """è·å–åˆ†å­éª¨æ¶"""
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    
    try:
        scaffold = MurckoScaffold.GetScaffoldForMol(mol)
        return Chem.MolToSmiles(scaffold)
    except:
        return None

# === æ•°æ®å¤„ç†å‡½æ•° ===

def clean_data_for_training(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, Dict[str, Any]]:
    """æ¸…ç†è®­ç»ƒæ•°æ®ï¼Œå¤„ç†NaNå€¼å’Œå¼‚å¸¸å€¼"""
    
    # æ•°æ®æ¸…ç†ä¿¡æ¯
    cleaning_info = {
        'original_samples': len(X),
        'removed_samples': 0,
        'final_samples': 0,
        'nan_in_features': 0,
        'nan_in_targets': 0
    }
    
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    if len(X) == 0 or len(y) == 0:
        st.error("æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
        return X, y, cleaning_info
    
    # æ£€æŸ¥ç‰¹å¾ä¸­çš„NaN
    nan_mask_X = np.isnan(X).any(axis=1)
    cleaning_info['nan_in_features'] = np.sum(nan_mask_X)
    
    # æ£€æŸ¥ç›®æ ‡å˜é‡ä¸­çš„NaN
    nan_mask_y = np.isnan(y)
    cleaning_info['nan_in_targets'] = np.sum(nan_mask_y)
    
    # åˆå¹¶æ‰€æœ‰éœ€è¦ç§»é™¤çš„æ ·æœ¬
    invalid_mask = nan_mask_X | nan_mask_y
    
    if np.any(invalid_mask):
        st.warning(f"æ£€æµ‹åˆ° {np.sum(invalid_mask)} ä¸ªåŒ…å«NaNå€¼çš„æ ·æœ¬ï¼Œå°†è¢«ç§»é™¤")
        
        # ç§»é™¤åŒ…å«NaNçš„æ ·æœ¬
        X_clean = X[~invalid_mask]
        y_clean = y[~invalid_mask]
        
        cleaning_info['removed_samples'] = np.sum(invalid_mask)
        cleaning_info['final_samples'] = len(X_clean)
        
        # æ£€æŸ¥æ¸…ç†åçš„æ•°æ®
        if len(X_clean) == 0:
            st.error("æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡")
            return X, y, cleaning_info
        
        if len(X_clean) < 10:
            st.warning(f"æ¸…ç†ååªæœ‰ {len(X_clean)} ä¸ªæ ·æœ¬ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½")
        
        st.success(f"æ•°æ®æ¸…ç†å®Œæˆï¼šä¿ç•™ {len(X_clean)} ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼ˆç§»é™¤ {np.sum(invalid_mask)} ä¸ªæ— æ•ˆæ ·æœ¬ï¼‰")
        
        return X_clean, y_clean, cleaning_info
    else:
        cleaning_info['final_samples'] = len(X)
        st.info("æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæ— éœ€æ¸…ç†")
        return X, y, cleaning_info

@st.cache_data
def preprocess_data(data: pd.DataFrame, feature_cols: List[str], 
                   target_col: str) -> Tuple[np.ndarray, np.ndarray]:
    """æ•°æ®é¢„å¤„ç†"""
    # ç§»é™¤ç¼ºå¤±å€¼
    clean_data = data.dropna(subset=feature_cols + [target_col])
    
    X = clean_data[feature_cols].values
    y = clean_data[target_col].values
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    return X_scaled, y, scaler

def validate_training_data(X: np.ndarray, y: np.ndarray) -> bool:
    """éªŒè¯è®­ç»ƒæ•°æ®çš„æœ‰æ•ˆæ€§"""
    
    # æ£€æŸ¥æ•°æ®å½¢çŠ¶
    if X.shape[0] != y.shape[0]:
        st.error(f"ç‰¹å¾çŸ©é˜µå’Œæ ‡ç­¾æ•°é‡ä¸åŒ¹é…ï¼šXæœ‰{X.shape[0]}ä¸ªæ ·æœ¬ï¼Œyæœ‰{y.shape[0]}ä¸ªæ ·æœ¬")
        return False
    
    # æ£€æŸ¥æœ€å°æ ·æœ¬æ•°
    if len(X) < 10:
        st.error(f"æ ·æœ¬æ•°é‡è¿‡å°‘ï¼ˆ{len(X)}ï¼‰ï¼Œéœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ")
        return False
    
    # æ£€æŸ¥æ ‡ç­¾çš„ç±»åˆ«
    unique_labels = np.unique(y)
    if len(unique_labels) < 2:
        st.error(f"æ ‡ç­¾ç±»åˆ«ä¸è¶³ï¼Œåªæœ‰ {len(unique_labels)} ä¸ªç±»åˆ«ï¼Œéœ€è¦è‡³å°‘2ä¸ªç±»åˆ«è¿›è¡Œåˆ†ç±»")
        return False
    
    # æ£€æŸ¥ç±»åˆ«å¹³è¡¡
    label_counts = pd.Series(y).value_counts()
    min_class_count = label_counts.min()
    if min_class_count < 2:
        st.warning(f"æœ€å°‘çš„ç±»åˆ«åªæœ‰ {min_class_count} ä¸ªæ ·æœ¬ï¼Œå¯èƒ½å½±å“äº¤å‰éªŒè¯")
    
    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
    st.info(f"è®­ç»ƒæ•°æ®ç»Ÿè®¡ï¼š{len(X)} ä¸ªæ ·æœ¬ï¼Œ{X.shape[1]} ä¸ªç‰¹å¾")
    
    col1, col2 = st.columns(2)
    with col1:
        for i, (label, count) in enumerate(label_counts.items()):
            st.metric(f"ç±»åˆ« {label}", f"{count} ä¸ªæ ·æœ¬")
    
    with col2:
        balance_ratio = min(label_counts) / max(label_counts)
        st.metric("ç±»åˆ«å¹³è¡¡æ¯”", f"{balance_ratio:.3f}")
    
    return True

# === æ¨¡å‹è®­ç»ƒå‡½æ•° ===

def create_neural_network(input_dim: int, hidden_layers: List[int] = [128, 64, 32]) -> keras.Model:
    """åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹"""
    model = keras.Sequential()
    model.add(layers.Dense(hidden_layers[0], activation='relu', input_dim=input_dim))
    model.add(layers.Dropout(0.3))
    
    for units in hidden_layers[1:]:
        model.add(layers.Dense(units, activation='relu'))
        model.add(layers.Dropout(0.3))
    
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    return model

@st.cache_data
def train_multiple_models(X_train: np.ndarray, X_test: np.ndarray, 
                         y_train: np.ndarray, y_test: np.ndarray,
                         selected_models: List[str]) -> Dict[str, Any]:
    """è®­ç»ƒå¤šä¸ªæ¨¡å‹å¹¶æ¯”è¾ƒæ€§èƒ½"""
    config = load_config()
    results = {}
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, model_name in enumerate(selected_models):
        status_text.text(f"æ­£åœ¨è®­ç»ƒ {model_name}...")
        
        if model_name == "ç¥ç»ç½‘ç»œ":
            model = create_neural_network(X_train.shape[1])
            history = model.fit(X_train, y_train, epochs=50, batch_size=32, 
                              validation_split=0.2, verbose=0)
            y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()
            y_prob = model.predict(X_test).flatten()
        else:
            model_class = config.models[model_name]
            
            # ä½¿ç”¨é»˜è®¤å‚æ•°å¿«é€Ÿè®­ç»ƒ
            if model_name == "éšæœºæ£®æ—":
                model = model_class(n_estimators=100, random_state=42)
            elif model_name == "æ”¯æŒå‘é‡æœº":
                model = model_class(kernel='rbf', probability=True, random_state=42)
            elif model_name == "XGBoost":
                model = model_class(n_estimators=100, random_state=42)
            elif model_name == "LightGBM":
                model = model_class(n_estimators=100, random_state=42, verbose=-1)
            
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            y_prob = model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        accuracy = accuracy_score(y_test, y_pred)
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        auc_score = auc(fpr, tpr)
        
        # äº¤å‰éªŒè¯
        if model_name != "ç¥ç»ç½‘ç»œ":
            cv_scores = cross_val_score(model, X_train, y_train, cv=5)
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
        else:
            cv_mean = cv_std = None
        
        results[model_name] = {
            'model': model,
            'accuracy': accuracy,
            'auc_score': auc_score,
            'fpr': fpr,
            'tpr': tpr,
            'y_pred': y_pred,
            'y_prob': y_prob,
            'cv_mean': cv_mean,
            'cv_std': cv_std
        }
        
        progress_bar.progress((i + 1) / len(selected_models))
    
    status_text.text("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    return results

# === å¯è§†åŒ–å‡½æ•° ===

def create_model_comparison_plot(results: Dict[str, Any]) -> go.Figure:
    """åˆ›å»ºæ¨¡å‹æ¯”è¾ƒå›¾è¡¨"""
    models = list(results.keys())
    accuracies = [results[model]['accuracy'] for model in models]
    auc_scores = [results[model]['auc_score'] for model in models]
    
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=('æ¨¡å‹å‡†ç¡®ç‡æ¯”è¾ƒ', 'æ¨¡å‹AUCæ¯”è¾ƒ'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # å‡†ç¡®ç‡æŸ±çŠ¶å›¾
    fig.add_trace(
        go.Bar(x=models, y=accuracies, name='å‡†ç¡®ç‡', 
               marker_color='lightblue', text=[f'{acc:.3f}' for acc in accuracies],
               textposition='auto'),
        row=1, col=1
    )
    
    # AUCæŸ±çŠ¶å›¾
    fig.add_trace(
        go.Bar(x=models, y=auc_scores, name='AUC', 
               marker_color='lightgreen', text=[f'{auc:.3f}' for auc in auc_scores],
               textposition='auto'),
        row=1, col=2
    )
    
    fig.update_layout(
        title_text="æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ",
        showlegend=False,
        height=400
    )
    
    return fig

def create_roc_comparison_plot(results: Dict[str, Any]) -> go.Figure:
    """åˆ›å»ºROCæ›²çº¿æ¯”è¾ƒå›¾"""
    fig = go.Figure()
    
    colors = px.colors.qualitative.Set1
    
    for i, (model_name, result) in enumerate(results.items()):
        fig.add_trace(go.Scatter(
            x=result['fpr'], 
            y=result['tpr'],
            mode='lines',
            name=f"{model_name} (AUC = {result['auc_score']:.3f})",
            line=dict(color=colors[i % len(colors)], width=2)
        ))
    
    # æ·»åŠ å¯¹è§’çº¿
    fig.add_trace(go.Scatter(
        x=[0, 1], 
        y=[0, 1],
        mode='lines',
        name='éšæœºåˆ†ç±»å™¨',
        line=dict(color='gray', dash='dash')
    ))
    
    fig.update_layout(
        title='ROCæ›²çº¿æ¯”è¾ƒ',
        xaxis_title='å‡æ­£ç‡ (FPR)',
        yaxis_title='çœŸæ­£ç‡ (TPR)',
        height=500,
        legend=dict(x=0.4, y=0.1)
    )
    
    return fig

def create_correlation_heatmap(data: pd.DataFrame) -> go.Figure:
    """åˆ›å»ºç›¸å…³æ€§çƒ­åŠ›å›¾"""
    numeric_cols = data.select_dtypes(include=[np.number]).columns
    corr_matrix = data[numeric_cols].corr()
    
    fig = ff.create_annotated_heatmap(
        z=corr_matrix.values,
        x=list(corr_matrix.columns),
        y=list(corr_matrix.columns),
        annotation_text=corr_matrix.round(2).values,
        colorscale='RdBu',
        zmid=0
    )
    
    fig.update_layout(
        title='ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾',
        height=600
    )
    
    return fig

def create_pca_plot(X: np.ndarray, y: np.ndarray) -> go.Figure:
    """åˆ›å»ºPCAé™ç»´å¯è§†åŒ–"""
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    
    fig = px.scatter(
        x=X_pca[:, 0], 
        y=X_pca[:, 1], 
        color=y.astype(str),
        title=f'PCAé™ç»´å¯è§†åŒ– (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.3f})',
        labels={'x': f'PC1 ({pca.explained_variance_ratio_[0]:.3f})',
                'y': f'PC2 ({pca.explained_variance_ratio_[1]:.3f})',
                'color': 'æ ‡ç­¾'}
    )
    
    return fig

# === é¡¹ç›®ç®¡ç†å‡½æ•° ===

def create_project_directory() -> str:
    """åˆ›å»ºé¡¹ç›®ç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    random_id = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
    project_name = f"{timestamp}_{random_id}"
    project_dir = os.path.join("./projects", project_name)
    os.makedirs(project_dir, exist_ok=True)
    return project_dir

def convert_numpy_types(obj):
    """å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œä»¥ä¾¿JSONåºåˆ—åŒ–"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

def save_project_results(project_dir: str, results: Dict[str, Any], 
                        data_info: Dict[str, Any]) -> None:
    """ä¿å­˜é¡¹ç›®ç»“æœ"""
    # ä¿å­˜æ¨¡å‹æ€§èƒ½ç»“æœ
    performance_data = []
    for model_name, result in results.items():
        performance_data.append({
            'Model': model_name,
            'Accuracy': float(result['accuracy']) if result['accuracy'] is not None else None,
            'AUC': float(result['auc_score']) if result['auc_score'] is not None else None,
            'CV_Mean': float(result.get('cv_mean')) if result.get('cv_mean') is not None else None,
            'CV_Std': float(result.get('cv_std')) if result.get('cv_std') is not None else None
        })
    
    performance_df = pd.DataFrame(performance_data)
    performance_df.to_csv(os.path.join(project_dir, 'model_performance.csv'), index=False)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    best_model_name = max(results.keys(), key=lambda k: results[k]['auc_score'])
    best_model = results[best_model_name]['model']
    
    try:
        joblib.dump(best_model, os.path.join(project_dir, 'best_model.pkl'))
    except Exception as e:
        st.error(f"ä¿å­˜æœ€ä½³æ¨¡å‹æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜æ‰€æœ‰æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    models_dir = os.path.join(project_dir, 'all_models')
    os.makedirs(models_dir, exist_ok=True)
    
    for model_name, result in results.items():
        try:
            model_filename = f"{model_name.replace(' ', '_')}_model.pkl"
            joblib.dump(result['model'], os.path.join(models_dir, model_filename))
        except Exception as e:
            st.warning(f"ä¿å­˜æ¨¡å‹ {model_name} æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜é¢„æµ‹ç»“æœï¼ˆç”¨äºåç»­åˆ†æï¼‰
    try:
        predictions_data = {}
        for model_name, result in results.items():
            # ç¡®ä¿è½¬æ¢ä¸ºPythonåŸç”Ÿåˆ—è¡¨ç±»å‹
            y_pred = result['y_pred']
            y_prob = result['y_prob']
            
            if hasattr(y_pred, 'tolist'):
                predictions_data[f'{model_name}_pred'] = y_pred.tolist()
            else:
                predictions_data[f'{model_name}_pred'] = list(y_pred)
                
            if hasattr(y_prob, 'tolist'):
                predictions_data[f'{model_name}_prob'] = y_prob.tolist()
            else:
                predictions_data[f'{model_name}_prob'] = list(y_prob)
        
        predictions_df = pd.DataFrame(predictions_data)
        predictions_df.to_csv(os.path.join(project_dir, 'predictions.csv'), index=False)
    except Exception as e:
        st.warning(f"ä¿å­˜é¢„æµ‹ç»“æœæ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜é¡¹ç›®ä¿¡æ¯ - ç¡®ä¿æ‰€æœ‰æ•°æ®ç±»å‹éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
    project_info = {
        'creation_time': datetime.now().isoformat(),
        'best_model': best_model_name,
        'best_auc': float(results[best_model_name]['auc_score']),
        'best_accuracy': float(results[best_model_name]['accuracy']),
        'models_count': len(results),
        'models_list': list(results.keys()),
        'data_info': convert_numpy_types(data_info)  # è½¬æ¢numpyç±»å‹
    }
    
    import json
    with open(os.path.join(project_dir, 'project_info.json'), 'w', encoding='utf-8') as f:
        json.dump(project_info, f, ensure_ascii=False, indent=2)

# === ä¸»åº”ç”¨ç•Œé¢ ===

def show_sidebar():
    """æ˜¾ç¤ºä¾§è¾¹æ """
    st.sidebar.markdown("""
    <div style="text-align: center; padding: 1rem;">
        <h2 style="color: black;">ğŸ§¬ CADDå¹³å°</h2>
        <p style="color: black;">è®¡ç®—æœºè¾…åŠ©è¯ç‰©è®¾è®¡</p>
    </div>
    """, unsafe_allow_html=True)
    
    # å¯¼èˆªèœå• - ç§»é™¤é¡¹ç›®ç®¡ç†å’Œé«˜çº§åˆ†æ
    menu_options = {
        "ğŸ  é¦–é¡µ": "é¦–é¡µ",
        "ğŸ“Š æ•°æ®å±•ç¤º": "æ•°æ®å±•ç¤º", 
        "ğŸ¤– æ¨¡å‹è®­ç»ƒ": "æ¨¡å‹è®­ç»ƒ",
        "ğŸ”¬ æ´»æ€§é¢„æµ‹": "æ´»æ€§é¢„æµ‹",
        "ğŸ“š çŸ¥è¯†è·å–": "çŸ¥è¯†è·å–"
    }
    
    selected = st.sidebar.selectbox(
        "é€‰æ‹©åŠŸèƒ½æ¨¡å—",
        list(menu_options.keys()),
        format_func=lambda x: x
    )
    
    return menu_options[selected]

def show_homepage():
    """æ˜¾ç¤ºé¦–é¡µ"""
    # ä¸»æ ‡é¢˜
    st.markdown("""
    <div class="main-container">
        <h1>ğŸ§¬ 2025 CADDè¯¾ç¨‹å®è·µå¹³å°</h1>
        <p style="font-size: 1.2em;">ç°ä»£åŒ–è®¡ç®—æœºè¾…åŠ©è¯ç‰©è®¾è®¡å·¥å…·å¥—ä»¶</p>
        <p>é›†æˆå¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹å’Œäº¤äº’å¼å¯è§†åŒ–</p>
    </div>
    """, unsafe_allow_html=True)
    
    # åŠŸèƒ½ä»‹ç»å¡ç‰‡
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="info-card">
            <h3>ğŸ“Š æ•°æ®å±•ç¤º</h3>
            <p>â€¢ å¤šç»´åº¦æ•°æ®å¯è§†åŒ–</p>
            <p>â€¢ ç›¸å…³æ€§åˆ†æ</p>
            <p>â€¢ PCAé™ç»´å¯è§†åŒ–</p>
            <p>â€¢ åˆ†å­æè¿°ç¬¦åˆ†æ</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="info-card">
            <h3>ğŸ¤– æ¨¡å‹è®­ç»ƒ</h3>
            <p>â€¢ å¤šæ¨¡å‹å¯¹æ¯”è®­ç»ƒ</p>
            <p>â€¢ è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–</p>
            <p>â€¢ äº¤å‰éªŒè¯è¯„ä¼°</p>
            <p>â€¢ æ€§èƒ½å¯è§†åŒ–åˆ†æ</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="info-card">
            <h3>ğŸ”¬ æ´»æ€§é¢„æµ‹</h3>
            <p>â€¢ å•åˆ†å­é¢„æµ‹</p>
            <p>â€¢ æ‰¹é‡é¢„æµ‹åˆ†æ</p>
            <p>â€¢ é¢„æµ‹ç½®ä¿¡åº¦è¯„ä¼°</p>
            <p>â€¢ æ–‡çŒ®æœç´¢åŠŸèƒ½</p>
        </div>
        """, unsafe_allow_html=True)
    
    # å¹³å°ç‰¹è‰²
    st.markdown("## ğŸš€ å¹³å°ç‰¹è‰²")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### æŠ€æœ¯ä¼˜åŠ¿
        - **å¤šæ¨¡å‹æ”¯æŒ**: RFã€SVMã€XGBã€LGBã€NN
        - **äº¤äº’å¼å¯è§†åŒ–**: PlotlyåŠ¨æ€å›¾è¡¨
        - **é«˜æ€§èƒ½è®¡ç®—**: å¹¶è¡Œå¤„ç†å’Œç¼“å­˜ä¼˜åŒ–
        - **æ¨¡å—åŒ–æ¶æ„**: æ˜“äºæ‰©å±•å’Œç»´æŠ¤
        """)
    
    with col2:
        st.markdown("""
        ### åŠŸèƒ½äº®ç‚¹
        - **æ™ºèƒ½åŒ–åˆ†æ**: è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹é€‰æ‹©
        - **åˆ†å­å¯è§†åŒ–**: åŒ–å­¦ç»“æ„ç¼–è¾‘å’Œå±•ç¤º
        - **æ–‡çŒ®è·å–**: è‡ªåŠ¨æœç´¢ç›¸å…³ç ”ç©¶è®ºæ–‡
        - **æ‰¹é‡å¤„ç†**: æ”¯æŒå¤§è§„æ¨¡æ•°æ®é¢„æµ‹
        """)

# ä¸»ç¨‹åºå…¥å£
def main():
    """ä¸»ç¨‹åº"""
    # æ˜¾ç¤ºä¾§è¾¹æ å¹¶è·å–é€‰æ‹©çš„é¡µé¢
    selected_page = show_sidebar()
    
    # æ ¹æ®é€‰æ‹©æ˜¾ç¤ºå¯¹åº”é¡µé¢
    if selected_page == "é¦–é¡µ":
        show_homepage()
    elif selected_page == "æ•°æ®å±•ç¤º":
        show_data_analysis()
    elif selected_page == "æ¨¡å‹è®­ç»ƒ":
        show_model_training()
    elif selected_page == "æ´»æ€§é¢„æµ‹":
        show_activity_prediction()
    elif selected_page == "çŸ¥è¯†è·å–":
        show_knowledge_acquisition()

def show_data_analysis():
    """æ•°æ®å±•ç¤ºæ¨¡å—"""
    st.title("ğŸ“Š æ•°æ®å±•ç¤ºä¸åˆ†æ")
    
    # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
    st.markdown("### ğŸ“ æ•°æ®ä¸Šä¼ ")
    
    col1, col2 = st.columns([2, 1])
    with col1:
        uploaded_file = st.file_uploader(
            "é€‰æ‹©CSVæ•°æ®æ–‡ä»¶", 
            type=['csv'],
            help="ä¸Šä¼ åŒ…å«SMILESå’Œæ´»æ€§æ ‡ç­¾çš„CSVæ–‡ä»¶"
        )
    
    with col2:
        st.markdown("""
        **æ•°æ®æ ¼å¼è¦æ±‚:**
        - CSVæ ¼å¼æ–‡ä»¶
        - åŒ…å«SMILESåˆ—
        - åŒ…å«æ´»æ€§æ ‡ç­¾åˆ—
        - å¯åŒ…å«å…¶ä»–ç‰¹å¾åˆ—
        """)
    
    # ç¤ºä¾‹æ•°æ®é€‰æ‹©
    if not uploaded_file:
        st.markdown("### ğŸ“‹ æˆ–é€‰æ‹©ç¤ºä¾‹æ•°æ®")
        csv_files = glob.glob("./data/*.csv")
        if csv_files:
            example_file = st.selectbox(
                "é€‰æ‹©ç¤ºä¾‹æ•°æ®é›†", 
                [os.path.basename(f) for f in csv_files]
            )
            selected_file = [f for f in csv_files if os.path.basename(f) == example_file][0]
            data = pd.read_csv(selected_file)
        else:
            st.warning("æœªæ‰¾åˆ°ç¤ºä¾‹æ•°æ®æ–‡ä»¶ï¼Œè¯·ä¸Šä¼ æ•°æ®æˆ–å°†CSVæ–‡ä»¶æ”¾å…¥./data/ç›®å½•")
            return
    else:
        data = pd.read_csv(uploaded_file)
    
    if 'data' in locals():
        # æ•°æ®åŸºæœ¬ä¿¡æ¯
        st.markdown("### ğŸ“ˆ æ•°æ®æ¦‚å†µ")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("æ ·æœ¬æ€»æ•°", len(data))
        with col2:
            st.metric("ç‰¹å¾æ•°é‡", len(data.columns))
        with col3:
            st.metric("ç¼ºå¤±å€¼", data.isnull().sum().sum())
        with col4:
            if len(data.select_dtypes(include=[np.number]).columns) > 0:
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                st.metric("æ•°å€¼å‹ç‰¹å¾", len(numeric_cols))
        
        # æ•°æ®é¢„è§ˆ
        st.markdown("### ğŸ” æ•°æ®é¢„è§ˆ")
        st.dataframe(data.head(10), use_container_width=True)
        
        # æ•°å€¼å‹ç‰¹å¾åˆ†æ
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        
        if numeric_cols:
            st.markdown("### ğŸ“Š æ•°å€¼å‹ç‰¹å¾åˆ†æ")
            
            # æè¿°æ€§ç»Ÿè®¡
            st.markdown("#### æè¿°æ€§ç»Ÿè®¡")
            st.dataframe(data[numeric_cols].describe(), use_container_width=True)
            
            # ç›¸å…³æ€§åˆ†æ
            if len(numeric_cols) > 1:
                st.markdown("#### ç‰¹å¾ç›¸å…³æ€§åˆ†æ")
                
                corr_matrix = data[numeric_cols].corr()
                
                # åˆ›å»ºäº¤äº’å¼çƒ­åŠ›å›¾
                fig = px.imshow(
                    corr_matrix,
                    labels=dict(x="ç‰¹å¾", y="ç‰¹å¾", color="ç›¸å…³ç³»æ•°"),
                    x=corr_matrix.columns,
                    y=corr_matrix.columns,
                    color_continuous_scale='RdBu_r',
                    aspect="auto",
                    title="ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾"
                )
                
                fig.update_layout(height=600)
                st.plotly_chart(fig, use_container_width=True)
        
        # åˆ†å­ç›¸å…³åˆ†æï¼ˆå¦‚æœæœ‰SMILESåˆ—ï¼‰
        smiles_cols = [col for col in data.columns if 'smiles' in col.lower()]
        if smiles_cols:
            st.markdown("### ğŸ§ª åˆ†å­åˆ†æ")
            
            smiles_col = st.selectbox("é€‰æ‹©SMILESåˆ—", smiles_cols)
            
            if st.button("è®¡ç®—åˆ†å­æè¿°ç¬¦"):
                with st.spinner("æ­£åœ¨è®¡ç®—åˆ†å­æè¿°ç¬¦..."):
                    descriptors_list = []
                    
                    progress_bar = st.progress(0)
                    
                    for i, smiles in enumerate(data[smiles_col]):
                        if pd.notna(smiles):
                            descriptors = calculate_molecular_descriptors(smiles)
                            if descriptors:
                                descriptors_list.append(descriptors)
                        
                        progress_bar.progress((i + 1) / len(data))
                    
                    if descriptors_list:
                        descriptors_df = pd.DataFrame(descriptors_list)
                        
                        st.success(f"æˆåŠŸè®¡ç®—äº† {len(descriptors_df)} ä¸ªåˆ†å­çš„æè¿°ç¬¦")
                        
                        # æè¿°ç¬¦ç»Ÿè®¡
                        st.markdown("#### åˆ†å­æè¿°ç¬¦ç»Ÿè®¡")
                        st.dataframe(descriptors_df.describe(), use_container_width=True)
                        
                        # Lipinskiäº”è§„åˆ™æ£€æŸ¥
                        st.markdown("#### Lipinskiäº”è§„åˆ™æ£€æŸ¥")
                        
                        lipinski_violations = []
                        for _, row in descriptors_df.iterrows():
                            violations = 0
                            if row.get('MolWt', 0) > 500:
                                violations += 1
                            if row.get('LogP', 0) > 5:
                                violations += 1
                            if row.get('NumHDonors', 0) > 5:
                                violations += 1
                            if row.get('NumHAcceptors', 0) > 10:
                                violations += 1
                            lipinski_violations.append(violations)
                        
                        violation_counts = pd.Series(lipinski_violations).value_counts().sort_index()
                        
                        fig = px.bar(
                            x=violation_counts.index,
                            y=violation_counts.values,
                            title='Lipinskiäº”è§„åˆ™è¿åæƒ…å†µ',
                            labels={'x': 'è¿åè§„åˆ™æ•°é‡', 'y': 'åˆ†å­æ•°é‡'}
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                        drug_like = sum([v == 0 for v in lipinski_violations])
                        st.metric(
                            "è¯ç‰©æ ·åˆ†å­æ•°é‡", 
                            f"{drug_like} / {len(lipinski_violations)} ({drug_like/len(lipinski_violations)*100:.1f}%)"
                        )

def show_model_training():
    st.title("ğŸ¤– å¤šæ¨¡å‹è®­ç»ƒä¸æ¯”è¾ƒ")
    
    # æ•°æ®åŠ è½½
    st.markdown("### ğŸ“ æ•°æ®å‡†å¤‡")
    
    uploaded_file = st.file_uploader(
        "é€‰æ‹©è®­ç»ƒæ•°æ®æ–‡ä»¶", 
        type=['csv'],
        key="training_data",
        help="ä¸Šä¼ åŒ…å«SMILESå’Œæ´»æ€§æ ‡ç­¾çš„CSVæ–‡ä»¶"
    )
    
    # ç¤ºä¾‹æ•°æ®é€‰æ‹©
    if not uploaded_file:
        csv_files = glob.glob("./data/*.csv")
        if csv_files:
            example_file = st.selectbox(
                "æˆ–é€‰æ‹©ç¤ºä¾‹æ•°æ®é›†", 
                [os.path.basename(f) for f in csv_files],
                key="training_example"
            )
            selected_file = [f for f in csv_files if os.path.basename(f) == example_file][0]
            data = pd.read_csv(selected_file)
        else:
            st.warning("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·ä¸Šä¼ è®­ç»ƒæ•°æ®")
            return
    else:
        data = pd.read_csv(uploaded_file)
    
    if 'data' in locals():
        st.success(f"æ•°æ®åŠ è½½æˆåŠŸï¼æ ·æœ¬æ•°é‡: {len(data)}, ç‰¹å¾æ•°é‡: {len(data.columns)}")
        
        # é€‰æ‹©åˆ—
        st.markdown("### ğŸ¯ ç‰¹å¾å’Œæ ‡ç­¾é€‰æ‹©")
        
        col1, col2 = st.columns(2)
        
        with col1:
            smiles_cols = [col for col in data.columns if 'smiles' in col.lower()]
            smiles_col = st.selectbox("é€‰æ‹©SMILESåˆ—", smiles_cols if smiles_cols else data.columns)
        
        with col2:
            label_cols = [col for col in data.columns if any(keyword in col.lower() for keyword in ['label', 'target', 'class', 'active'])]
            label_col = st.selectbox("é€‰æ‹©æ ‡ç­¾åˆ—", label_cols if label_cols else data.columns)
        
        # æ£€æŸ¥æ•°æ®
        if smiles_col and label_col:
            # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
            label_counts = data[label_col].value_counts()
            st.markdown("#### æ ‡ç­¾åˆ†å¸ƒ")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("æ­£æ ·æœ¬", int(label_counts.get(1, 0)))
            with col2:
                st.metric("è´Ÿæ ·æœ¬", int(label_counts.get(0, 0)))
            with col3:
                if len(label_counts) > 0:
                    ratio = min(label_counts) / max(label_counts)
                    st.metric("ç±»åˆ«å¹³è¡¡æ¯”", f"{ratio:.3f}")
            
            # ç‰¹å¾å·¥ç¨‹
            st.markdown("### ğŸ”§ ç‰¹å¾å·¥ç¨‹")
            
            fingerprint_type = st.selectbox(
                "é€‰æ‹©åˆ†å­æŒ‡çº¹ç±»å‹",
                ["morgan", "maccs"],
                help="MorganæŒ‡çº¹æ›´é€‚åˆç»“æ„ç›¸ä¼¼æ€§"
            )
            
            if st.button("å¼€å§‹ç‰¹å¾æå–", type="primary"):
                with st.spinner("æ­£åœ¨æå–åˆ†å­ç‰¹å¾..."):
                    fingerprints = []
                    valid_indices = []
                    
                    progress_bar = st.progress(0)
                    
                    for i, smiles in enumerate(data[smiles_col]):
                        if pd.notna(smiles):
                            fp = mol_to_fingerprint(smiles, fingerprint_type)
                            if fp is not None:
                                fingerprints.append(fp)
                                valid_indices.append(i)
                        
                        progress_bar.progress((i + 1) / len(data))
                    
                    if fingerprints:
                        X = np.array(fingerprints)
                        y = data.iloc[valid_indices][label_col].values
                        
                        # æ£€æŸ¥æå–çš„ç‰¹å¾å’Œæ ‡ç­¾
                        st.markdown("#### ğŸ“‹ ç‰¹å¾æå–ç»“æœ")
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("æœ‰æ•ˆæ ·æœ¬æ•°", len(X))
                        with col2:
                            st.metric("ç‰¹å¾ç»´åº¦", X.shape[1])
                        with col3:
                            nan_count = np.sum(np.isnan(y))
                            st.metric("æ ‡ç­¾ä¸­NaNæ•°é‡", nan_count)
                        
                        # æ£€æŸ¥æ ‡ç­¾æ•°æ®è´¨é‡
                        if np.any(np.isnan(y)):
                            st.warning(f"æ£€æµ‹åˆ°æ ‡ç­¾ä¸­æœ‰ {np.sum(np.isnan(y))} ä¸ªNaNå€¼ï¼Œè®­ç»ƒæ—¶å°†è‡ªåŠ¨å¤„ç†")
                        
                        # æ£€æŸ¥ç‰¹å¾æ•°æ®è´¨é‡
                        nan_features = np.sum(np.isnan(X).any(axis=1))
                        if nan_features > 0:
                            st.warning(f"æ£€æµ‹åˆ° {nan_features} ä¸ªæ ·æœ¬çš„ç‰¹å¾åŒ…å«NaNå€¼ï¼Œè®­ç»ƒæ—¶å°†è‡ªåŠ¨å¤„ç†")
                        
                        st.success(f"ç‰¹å¾æå–å®Œæˆï¼å‡†å¤‡è®­ç»ƒæ•°æ®...")
                        
                        # å­˜å‚¨åˆ°session state
                        st.session_state['X'] = X
                        st.session_state['y'] = y
                        st.session_state['fingerprint_type'] = fingerprint_type
                    else:
                        st.error("ç‰¹å¾æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥SMILESæ•°æ®è´¨é‡")
            
            # æ¨¡å‹è®­ç»ƒ
            if 'X' in st.session_state and 'y' in st.session_state:
                st.markdown("### ğŸ¯ æ¨¡å‹è®­ç»ƒé…ç½®")
                
                X = st.session_state['X']
                y = st.session_state['y']
                
                # æ¨¡å‹é€‰æ‹©
                config = load_config()
                
                col1, col2 = st.columns(2)
                
                with col1:
                    selected_models = st.multiselect(
                        "é€‰æ‹©è¦è®­ç»ƒçš„æ¨¡å‹",
                        list(config.models.keys()),
                        default=["éšæœºæ£®æ—", "XGBoost"],
                        help="å»ºè®®é€‰æ‹©2-4ä¸ªæ¨¡å‹è¿›è¡Œæ¯”è¾ƒ"
                    )
                
                with col2:
                    test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.4, 0.2, 0.05)
                
                # å¼€å§‹è®­ç»ƒ
                if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary"):
                    if selected_models:
                        # æ•°æ®æ¸…ç†å’ŒéªŒè¯
                        st.markdown("#### ğŸ§¹ æ•°æ®æ¸…ç†")
                        X_clean, y_clean, cleaning_info = clean_data_for_training(X, y)
                        
                        # éªŒè¯æ¸…ç†åçš„æ•°æ®
                        if validate_training_data(X_clean, y_clean):
                            try:
                                # å°è¯•åˆ†å±‚æŠ½æ ·ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨æ™®é€šæŠ½æ ·
                                try:
                                    X_train, X_test, y_train, y_test = train_test_split(
                                        X_clean, y_clean, 
                                        test_size=test_size, 
                                        random_state=42, 
                                        stratify=y_clean
                                    )
                                except ValueError as e:
                                    st.warning(f"åˆ†å±‚æŠ½æ ·å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šéšæœºæŠ½æ ·: {e}")
                                    X_train, X_test, y_train, y_test = train_test_split(
                                        X_clean, y_clean, 
                                        test_size=test_size, 
                                        random_state=42
                                    )
                                
                                # å¼€å§‹è®­ç»ƒæ¨¡å‹
                                results = train_multiple_models(
                                    X_train, X_test, y_train, y_test,
                                    selected_models
                                )
                            except Exception as e:
                                st.error(f"æ•°æ®åˆ†å‰²å¤±è´¥: {e}")
                                st.stop()
                        else:
                            st.error("æ•°æ®éªŒè¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")
                            st.stop()
                        
                        # è‡ªåŠ¨ä¿å­˜è®­ç»ƒç»“æœ
                        st.markdown("#### ğŸ’¾ è‡ªåŠ¨ä¿å­˜æ¨¡å‹")
                        with st.spinner("æ­£åœ¨ä¿å­˜è®­ç»ƒç»“æœ..."):
                            project_dir = create_project_directory()
                            project_info = {
                                'fingerprint_type': fingerprint_type,
                                'test_size': float(test_size),  # ç¡®ä¿æ˜¯floatç±»å‹
                                'cleaning_info': convert_numpy_types(cleaning_info)  # è½¬æ¢numpyç±»å‹
                            }
                            save_project_results(project_dir, results, project_info)
                            
                            # åŒæ—¶ä¿å­˜é¢„å¤„ç†ä¿¡æ¯ï¼Œç”¨äºé¢„æµ‹æ—¶çš„ä¸€è‡´æ€§
                            preprocessing_info = {
                                'fingerprint_type': fingerprint_type,
                                'feature_shape': int(X_clean.shape[1]),  # è½¬æ¢ä¸ºint
                                'original_samples': int(cleaning_info['original_samples']),  # è½¬æ¢ä¸ºint
                                'final_samples': int(cleaning_info['final_samples'])  # è½¬æ¢ä¸ºint
                            }
                            
                            import json
                            with open(os.path.join(project_dir, 'preprocessing_info.json'), 'w', encoding='utf-8') as f:
                                json.dump(preprocessing_info, f, ensure_ascii=False, indent=2)
                        
                        st.success(f"âœ… æ¨¡å‹å·²è‡ªåŠ¨ä¿å­˜åˆ°: {project_dir}")
                        st.session_state['training_results'] = results
                        st.session_state['current_project_dir'] = project_dir
                        
                        # æ˜¾ç¤ºæ€§èƒ½æ¯”è¾ƒ
                        st.markdown("### ğŸ“Š æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
                        
                        # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹ä¿¡æ¯
                        best_model_name = max(results.keys(), key=lambda k: results[k]['auc_score'])
                        best_auc = results[best_model_name]['auc_score']
                        
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ğŸ† æœ€ä½³æ¨¡å‹", best_model_name)
                        with col2:
                            st.metric("ğŸ¯ æœ€ä½³AUC", f"{best_auc:.4f}")
                        with col3:
                            st.metric("ğŸ“ é¡¹ç›®è·¯å¾„", os.path.basename(project_dir))
                        
                        # ROCæ›²çº¿æ¯”è¾ƒ
                        roc_fig = create_roc_comparison_plot(results)
                        st.plotly_chart(roc_fig, use_container_width=True)
                        
                        # æ¨¡å‹æ¯”è¾ƒå›¾è¡¨
                        comparison_fig = create_model_comparison_plot(results)
                        st.plotly_chart(comparison_fig, use_container_width=True)
                        
                        # é¢å¤–ä¿å­˜é€‰é¡¹
                        st.markdown("#### ğŸ“‹ é¢å¤–æ“ä½œ")
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            if st.button("ğŸ” è·³è½¬åˆ°æ´»æ€§é¢„æµ‹"):
                                st.session_state['selected_project_for_prediction'] = os.path.basename(project_dir)
                                st.experimental_rerun()
                        
                        with col2:
                            if st.button("ğŸ“Š æŸ¥çœ‹é¡¹ç›®è¯¦æƒ…"):
                                st.info(f"é¡¹ç›®å·²ä¿å­˜ï¼Œå¯åœ¨é¡¹ç›®ç®¡ç†ä¸­æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
                        
                        # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶åˆ—è¡¨
                        st.markdown("#### ğŸ“‚ å·²ä¿å­˜æ–‡ä»¶")
                        saved_files = os.listdir(project_dir)
                        for file in saved_files:
                            file_path = os.path.join(project_dir, file)
                            file_size = os.path.getsize(file_path)
                            st.text(f"ğŸ“„ {file} ({file_size} bytes)")
                    else:
                        st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒ")

def show_activity_prediction():
    st.title("ğŸ”¬ åˆ†å­æ´»æ€§é¢„æµ‹")
    
    # åŠ è½½å·²è®­ç»ƒçš„é¡¹ç›®
    st.markdown("### ğŸ“‚ é€‰æ‹©é¢„æµ‹æ¨¡å‹")
    
    # åˆ›å»ºprojectsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs('./projects', exist_ok=True)
    
    projects = glob.glob('./projects/*')
    if not projects:
        st.warning("æœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒæ¨¡å‹")
        st.info("è¯·å‰å¾€ 'ğŸ¤– æ¨¡å‹è®­ç»ƒ' é¡µé¢åˆ›å»ºæ¨¡å‹")
        return
    
    # è·å–é¡¹ç›®ä¿¡æ¯å¹¶æŒ‰æ—¶é—´æ’åº
    project_info_list = []
    for project_path in projects:
        project_name = os.path.basename(project_path)
        info_file = os.path.join(project_path, 'project_info.json')
        
        if os.path.exists(info_file):
            try:
                import json
                with open(info_file, 'r', encoding='utf-8') as f:
                    info = json.load(f)
                
                project_info_list.append({
                    'name': project_name,
                    'path': project_path,
                    'creation_time': info.get('creation_time', ''),
                    'best_model': info.get('best_model', 'Unknown'),
                    'best_auc': info.get('best_auc', 0),
                    'valid': True
                })
            except:
                project_info_list.append({
                    'name': project_name,
                    'path': project_path,
                    'creation_time': '',
                    'best_model': 'Unknown',
                    'best_auc': 0,
                    'valid': False
                })
        else:
            project_info_list.append({
                'name': project_name,
                'path': project_path,
                'creation_time': '',
                'best_model': 'Unknown',
                'best_auc': 0,
                'valid': False
            })
    
    # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    project_info_list.sort(key=lambda x: x['creation_time'], reverse=True)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä»è®­ç»ƒæ¨¡å—è·³è½¬è¿‡æ¥çš„é¡¹ç›®
    default_index = 0
    if 'selected_project_for_prediction' in st.session_state:
        target_project = st.session_state['selected_project_for_prediction']
        for i, project_info in enumerate(project_info_list):
            if project_info['name'] == target_project:
                default_index = i
                break
        # æ¸…é™¤session state
        del st.session_state['selected_project_for_prediction']
    
    # æ˜¾ç¤ºé¡¹ç›®é€‰æ‹©
    if project_info_list:
        # åˆ›å»ºé¡¹ç›®æ˜¾ç¤ºé€‰é¡¹
        project_options = []
        for info in project_info_list:
            if info['valid']:
                option_text = f"ğŸŸ¢ {info['name']} | {info['best_model']} | AUC: {info['best_auc']:.4f}"
            else:
                option_text = f"ğŸ”´ {info['name']} | ä¿¡æ¯ä¸å®Œæ•´"
            project_options.append(option_text)
        
        selected_index = st.selectbox(
            "é€‰æ‹©è®­ç»ƒå¥½çš„é¡¹ç›®", 
            range(len(project_options)),
            format_func=lambda x: project_options[x],
            index=default_index
        )
        
        selected_project_info = project_info_list[selected_index]
        project_dir = selected_project_info['path']
        
        # æ˜¾ç¤ºé¡¹ç›®è¯¦ç»†ä¿¡æ¯
        if selected_project_info['valid']:
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ğŸ† æœ€ä½³æ¨¡å‹", selected_project_info['best_model'])
            with col2:
                st.metric("ğŸ¯ AUCå¾—åˆ†", f"{selected_project_info['best_auc']:.4f}")
            with col3:
                creation_time = selected_project_info['creation_time'][:19] if selected_project_info['creation_time'] else "Unknown"
                st.metric("â° åˆ›å»ºæ—¶é—´", creation_time)
            with col4:
                # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
                model_file = os.path.join(project_dir, 'best_model.pkl')
                model_status = "âœ… å¯ç”¨" if os.path.exists(model_file) else "âŒ ç¼ºå¤±"
                st.metric("ğŸ“ æ¨¡å‹æ–‡ä»¶", model_status)
    else:
        st.error("æ— æœ‰æ•ˆé¡¹ç›®")
        return
    
    # åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†ä¿¡æ¯
    model_file = os.path.join(project_dir, 'best_model.pkl')
    preprocessing_file = os.path.join(project_dir, 'preprocessing_info.json')
    
    if not os.path.exists(model_file):
        st.error("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é‡æ–°è®­ç»ƒæ¨¡å‹")
        return
    
    try:
        # åŠ è½½æ¨¡å‹
        with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
            model = joblib.load(model_file)
        
        # åŠ è½½é¢„å¤„ç†ä¿¡æ¯
        preprocessing_info = {}
        if os.path.exists(preprocessing_file):
            try:
                import json
                with open(preprocessing_file, 'r', encoding='utf-8') as f:
                    preprocessing_info = json.load(f)
            except:
                st.warning("âš ï¸ æ— æ³•åŠ è½½é¢„å¤„ç†ä¿¡æ¯ï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®")
                preprocessing_info = {'fingerprint_type': 'morgan'}
        else:
            st.warning("âš ï¸ é¢„å¤„ç†ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤è®¾ç½®")
            preprocessing_info = {'fingerprint_type': 'morgan'}
        
        # æ˜¾ç¤ºåŠ è½½ä¿¡æ¯
        col1, col2 = st.columns(2)
        with col1:
            st.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        with col2:
            fingerprint_type = preprocessing_info.get('fingerprint_type', 'morgan')
            st.info(f"ğŸ§¬ æŒ‡çº¹ç±»å‹: {fingerprint_type}")
        
    except Exception as e:
        st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        st.info("å¯èƒ½çš„åŸå› ï¼šæ¨¡å‹æ–‡ä»¶æŸåæˆ–ç‰ˆæœ¬ä¸å…¼å®¹")
        return
    
    # é¢„æµ‹é€‰é¡¹
    prediction_mode = st.radio(
        "é€‰æ‹©é¢„æµ‹æ¨¡å¼",
        ["å•åˆ†å­é¢„æµ‹", "æ‰¹é‡é¢„æµ‹"],
        horizontal=True
    )
    
    if prediction_mode == "å•åˆ†å­é¢„æµ‹":
        st.markdown("### ğŸ§ª å•åˆ†å­é¢„æµ‹")
        
        # åˆ†å­è¾“å…¥
        smiles_input = st.text_input(
            "è¾“å…¥åˆ†å­SMILES",
            value="CC(C)CC1=CC=C(C=C1)C(C)C(=O)O",
            help="è¾“å…¥è¦é¢„æµ‹çš„åˆ†å­çš„SMILESå­—ç¬¦ä¸²"
        )
        
        # åˆ†å­ç¼–è¾‘å™¨
        smile_code = st_ketcher(smiles_input)
        st.markdown(f"**å½“å‰åˆ†å­SMILES:** `{smile_code}`")
        
        if smile_code and st.button("ğŸ” å¼€å§‹é¢„æµ‹", type="primary"):
            with st.spinner("æ­£åœ¨è¿›è¡Œé¢„æµ‹..."):
                # ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„æŒ‡çº¹ç±»å‹
                fingerprint_type = preprocessing_info.get('fingerprint_type', 'morgan')
                fingerprint = mol_to_fingerprint(smile_code, fingerprint_type)
                
                if fingerprint is not None:
                    # è¿›è¡Œé¢„æµ‹
                    try:
                        if hasattr(model, 'predict_proba'):
                            prediction_prob = model.predict_proba([fingerprint])[0]
                            prediction = model.predict([fingerprint])[0]
                            confidence = max(prediction_prob)
                            active_prob = prediction_prob[1]
                        else:
                            prediction_prob = model.predict([fingerprint])[0]
                            prediction = 1 if prediction_prob > 0.5 else 0
                            confidence = prediction_prob if prediction == 1 else (1 - prediction_prob)
                            active_prob = prediction_prob
                        
                        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                        st.markdown("### ğŸ“‹ é¢„æµ‹ç»“æœ")
                        
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            result_text = "æ´»æ€§" if prediction == 1 else "éæ´»æ€§"
                            st.metric("é¢„æµ‹ç»“æœ", result_text)
                        
                        with col2:
                            st.metric("æ´»æ€§æ¦‚ç‡", f"{active_prob:.3f}")
                        
                        with col3:
                            st.metric("é¢„æµ‹ç½®ä¿¡åº¦", f"{confidence:.3f}")
                        
                        # åˆ†å­æè¿°ç¬¦
                        st.markdown("### ğŸ§® åˆ†å­æè¿°ç¬¦")
                        descriptors = calculate_molecular_descriptors(smile_code)
                        if descriptors:
                            desc_df = pd.DataFrame([descriptors])
                            st.dataframe(desc_df, use_container_width=True)
                        
                    except Exception as e:
                        st.error(f"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                else:
                    st.error("æ— æ³•è§£æè¯¥SMILESå­—ç¬¦ä¸²ï¼Œè¯·æ£€æŸ¥è¾“å…¥")
    
    elif prediction_mode == "æ‰¹é‡é¢„æµ‹":
        st.markdown("### ğŸ“Š æ‰¹é‡é¢„æµ‹")
        
        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ åŒ…å«SMILESçš„CSVæ–‡ä»¶",
            type=['csv'],
            help="CSVæ–‡ä»¶åº”åŒ…å«ä¸€åˆ—SMILESæ•°æ®"
        )
        
        if uploaded_file:
            batch_data = pd.read_csv(uploaded_file)
            st.write("æ•°æ®é¢„è§ˆ:")
            st.dataframe(batch_data.head(), use_container_width=True)
            
            # é€‰æ‹©SMILESåˆ—
            smiles_columns = [col for col in batch_data.columns if 'smiles' in col.lower()]
            if not smiles_columns:
                smiles_columns = batch_data.columns.tolist()
            
            smiles_col = st.selectbox("é€‰æ‹©SMILESåˆ—", smiles_columns)
            
            if st.button("ğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹", type="primary"):
                with st.spinner("æ­£åœ¨è¿›è¡Œæ‰¹é‡é¢„æµ‹..."):                        
                    predictions = []
                    probabilities = []
                    valid_indices = []
                    
                    progress_bar = st.progress(0)
                    
                    # ä½¿ç”¨è®­ç»ƒæ—¶ç›¸åŒçš„æŒ‡çº¹ç±»å‹
                    fingerprint_type = preprocessing_info.get('fingerprint_type', 'morgan')
                    
                    for i, smiles in enumerate(batch_data[smiles_col]):
                        if pd.notna(smiles):
                            fingerprint = mol_to_fingerprint(smiles, fingerprint_type)
                            if fingerprint is not None:
                                try:
                                    if hasattr(model, 'predict_proba'):
                                        pred_prob = model.predict_proba([fingerprint])[0]
                                        pred = model.predict([fingerprint])[0]
                                        prob = pred_prob[1]
                                    else:
                                        pred_prob = model.predict([fingerprint])[0]
                                        pred = 1 if pred_prob > 0.5 else 0
                                        prob = pred_prob
                                    
                                    predictions.append(pred)
                                    probabilities.append(prob)
                                    valid_indices.append(i)
                                except:
                                    pass
                        
                        progress_bar.progress((i + 1) / len(batch_data))
                    
                    if predictions:
                        # åˆ›å»ºç»“æœæ•°æ®æ¡†
                        results_data = batch_data.iloc[valid_indices].copy()
                        results_data['é¢„æµ‹ç»“æœ'] = predictions
                        results_data['æ´»æ€§æ¦‚ç‡'] = probabilities
                        results_data['é¢„æµ‹æ ‡ç­¾'] = ['æ´»æ€§' if p == 1 else 'éæ´»æ€§' for p in predictions]
                        
                        st.success(f"æ‰¹é‡é¢„æµ‹å®Œæˆï¼æˆåŠŸé¢„æµ‹ {len(predictions)} ä¸ªåˆ†å­")
                        
                        # æ˜¾ç¤ºé¢„æµ‹ç»Ÿè®¡
                        st.markdown("### ğŸ“ˆ é¢„æµ‹ç»Ÿè®¡")
                        
                        col1, col2, col3, col4 = st.columns(4)
                        
                        with col1:
                            st.metric("æ€»é¢„æµ‹æ•°", len(predictions))
                        with col2:
                            active_count = sum(predictions)
                            st.metric("é¢„æµ‹æ´»æ€§", active_count)
                        with col3:
                            inactive_count = len(predictions) - active_count
                            st.metric("é¢„æµ‹éæ´»æ€§", inactive_count)
                        with col4:
                            avg_prob = np.mean(probabilities)
                            st.metric("å¹³å‡æ´»æ€§æ¦‚ç‡", f"{avg_prob:.3f}")
                        
                        # æ¦‚ç‡åˆ†å¸ƒå›¾
                        fig = px.histogram(
                            x=probabilities,
                            nbins=20,
                            title="æ´»æ€§æ¦‚ç‡åˆ†å¸ƒ",
                            labels={'x': 'æ´»æ€§æ¦‚ç‡', 'y': 'åˆ†å­æ•°é‡'}
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # æ˜¾ç¤ºç»“æœè¡¨æ ¼
                        st.markdown("### ğŸ“‹ é¢„æµ‹ç»“æœ")
                        st.dataframe(results_data, use_container_width=True)
                        
                        # å¯¼å‡ºç»“æœ
                        csv_data = results_data.to_csv(index=False)
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½é¢„æµ‹ç»“æœ",
                            data=csv_data,
                            file_name=f"prediction_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv"
                        )
                    else:
                        st.error("æ‰¹é‡é¢„æµ‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥SMILESæ•°æ®è´¨é‡")

def show_knowledge_acquisition():
    """çŸ¥è¯†è·å–ä¸æ–‡çŒ®åˆ†ææ¨¡å—"""
    st.title("ğŸ“š çŸ¥è¯†è·å–ä¸æ–‡çŒ®åˆ†æ")
    
    # è®¾ç½®Entrezé‚®ç®±
    Entrez.email = "cadd@example.com"
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        search_keyword = st.text_input(
            "è¾“å…¥æœç´¢å…³é”®è¯",
            value="drug toxicity machine learning",
            help="è¾“å…¥è‹±æ–‡å…³é”®è¯è¿›è¡Œæ–‡çŒ®æœç´¢"
        )
    
    with col2:
        max_results = st.selectbox(
            "æœ€å¤§æœç´¢ç»“æœæ•°",
            [5, 10, 15, 20],
            index=0
        )
    
    if st.button("ğŸ” æœç´¢æ–‡çŒ®", type="primary"):
        with st.spinner("æ­£åœ¨æœç´¢æ–‡çŒ®..."):
            try:
                # æœç´¢PMCæ•°æ®åº“
                handle = Entrez.esearch(
                    db="pubmed", 
                    term=search_keyword, 
                    retmode="xml", 
                    retmax=max_results
                )
                search_results = Entrez.read(handle)
                pmid_list = search_results["IdList"]
                
                if pmid_list:
                    st.success(f"æ‰¾åˆ° {len(pmid_list)} ç¯‡ç›¸å…³æ–‡çŒ®")
                    
                    # è·å–æ–‡çŒ®è¯¦ç»†ä¿¡æ¯
                    handle = Entrez.efetch(
                        db="pubmed",
                        id=",".join(pmid_list),
                        rettype="abstract",
                        retmode="xml"
                    )
                    articles = Entrez.read(handle)
                    
                    # å­˜å‚¨æ–‡çŒ®ä¿¡æ¯åˆ°session state
                    st.session_state['articles'] = articles
                    st.session_state['pmid_list'] = pmid_list
                    
                    # æ˜¾ç¤ºæ–‡çŒ®åˆ—è¡¨
                    st.markdown("### ğŸ“„ æœç´¢ç»“æœ")
                    
                    article_options = []
                    for i, article in enumerate(articles['PubmedArticle']):
                        try:
                            title = article['MedlineCitation']['Article']['ArticleTitle']
                            authors = article['MedlineCitation']['Article']['AuthorList']
                            if authors:
                                first_author = authors[0]['LastName'] + " " + authors[0].get('ForeName', '')
                            else:
                                first_author = "Unknown"
                            
                            # é™åˆ¶æ ‡é¢˜é•¿åº¦
                            short_title = title[:100] + "..." if len(title) > 100 else title
                            article_options.append(f"{i+1}. {first_author} - {short_title}")
                        except:
                            article_options.append(f"{i+1}. æ–‡çŒ®ä¿¡æ¯è·å–å¤±è´¥")
                    
                    # è®©ç”¨æˆ·é€‰æ‹©è¦æŸ¥çœ‹abstractçš„æ–‡çŒ®
                    selected_articles = st.multiselect(
                        "é€‰æ‹©è¦æŸ¥çœ‹æ‘˜è¦çš„æ–‡çŒ®ï¼ˆå¯å¤šé€‰ï¼‰",
                        range(len(article_options)),
                        format_func=lambda x: article_options[x],
                        default=list(range(min(3, len(article_options))))  # é»˜è®¤é€‰æ‹©å‰3ç¯‡
                    )
                    
                    # æ˜¾ç¤ºé€‰ä¸­æ–‡çŒ®çš„è¯¦ç»†ä¿¡æ¯
                    if selected_articles:
                        st.markdown("### ğŸ“– æ–‡çŒ®æ‘˜è¦")
                        
                        for idx in selected_articles:
                            article = articles['PubmedArticle'][idx]
                            
                            try:
                                # æå–æ–‡çŒ®ä¿¡æ¯
                                pmid = pmid_list[idx]
                                title = article['MedlineCitation']['Article']['ArticleTitle']
                                
                                # ä½œè€…ä¿¡æ¯
                                authors = article['MedlineCitation']['Article']['AuthorList']
                                author_names = []
                                for author in authors[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªä½œè€…
                                    if 'LastName' in author and 'ForeName' in author:
                                        author_names.append(f"{author['LastName']} {author['ForeName']}")
                                
                                if len(authors) > 5:
                                    author_str = ", ".join(author_names) + ", et al."
                                else:
                                    author_str = ", ".join(author_names)
                                
                                # æœŸåˆŠä¿¡æ¯
                                journal = article['MedlineCitation']['Article']['Journal']['Title']
                                
                                # å‘è¡¨æ—¥æœŸ
                                pub_date = article['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']
                                year = pub_date.get('Year', 'Unknown')
                                
                                # æ‘˜è¦
                                abstract_text = ""
                                if 'Abstract' in article['MedlineCitation']['Article']:
                                    abstract_list = article['MedlineCitation']['Article']['Abstract']['AbstractText']
                                    if isinstance(abstract_list, list):
                                        abstract_text = " ".join([str(abs_part) for abs_part in abstract_list])
                                    else:
                                        abstract_text = str(abstract_list)
                                
                                # æ˜¾ç¤ºæ–‡çŒ®ä¿¡æ¯
                                with st.expander(f"ğŸ“„ {title[:100]}{'...' if len(title) > 100 else ''}", expanded=True):
                                    st.markdown(f"**æ ‡é¢˜**: {title}")
                                    st.markdown(f"**ä½œè€…**: {author_str}")
                                    st.markdown(f"**æœŸåˆŠ**: {journal}")
                                    st.markdown(f"**å¹´ä»½**: {year}")
                                    st.markdown(f"**PMID**: {pmid}")
                                    
                                    if abstract_text:
                                        st.markdown("**æ‘˜è¦**:")
                                        st.write(abstract_text)
                                    else:
                                        st.write("*è¯¥æ–‡çŒ®æ²¡æœ‰å¯ç”¨çš„æ‘˜è¦*")
                                    
                                    # PubMedé“¾æ¥
                                    pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                                    st.markdown(f"ğŸ”— [åœ¨PubMedä¸­æŸ¥çœ‹]({pubmed_url})")
                            
                            except Exception as e:
                                st.error(f"å¤„ç†ç¬¬{idx+1}ç¯‡æ–‡çŒ®æ—¶å‡ºé”™: {e}")
                
                else:
                    st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯")
                
            except Exception as e:
                st.error(f"æœç´¢å¤±è´¥: {e}")
                st.info("å¯èƒ½çš„åŸå› ï¼šç½‘ç»œè¿æ¥é—®é¢˜æˆ–NCBIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
    
    # æœç´¢å»ºè®®
    st.markdown("### ğŸ’¡ æœç´¢å»ºè®®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        **çƒ­é—¨æœç´¢å…³é”®è¯**:
        - "machine learning drug discovery"
        - "QSAR molecular toxicity"  
        - "deep learning ADMET"
        - "virtual screening compounds"
        - "molecular descriptors prediction"
        """)
    
    with col2:
        st.markdown("""
        **æœç´¢æŠ€å·§**:
        - ä½¿ç”¨è‹±æ–‡å…³é”®è¯
        - ç»„åˆå¤šä¸ªç›¸å…³æœ¯è¯­
        - ä½¿ç”¨å¼•å·åŒ…å«ç²¾ç¡®çŸ­è¯­
        - å°è¯•ä¸åŒçš„åŒä¹‰è¯
        - æ·»åŠ å¹´ä»½é™åˆ¶ï¼ˆå¦‚ "2020:2024[pdat]"ï¼‰
        """)

if __name__ == "__main__":
    main()